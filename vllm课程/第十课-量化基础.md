### 第10课-vLLM 量化基础知识

深度学习模型运行主流芯片：CPU、GPU、FPGA和专用ASIC四类芯片

资源压力：存储容量、内存带宽、功耗预算和实时性要求，推理效率成为决定成败的关键瓶颈。

模型量化作为业界公认的通用优化手段之一，能够将FP32模型高效转换为低比特定点数表示（典型为INT8），在几乎不牺牲精度的前提下，实现：

- 模型存储体积压缩约4倍
- 显存/内存占用降低至原25%
- 推理吞吐量提升2–4倍（得益于整数运算单元的高并行度）

核心流程分为两个步骤：

1. **量化转换阶段**：通过线性映射（通常配合一个缩放因子 S 和零点偏移 Z），把连续分布的 FP32 权重值对称或非对称地压缩到 [-128, 127] 或 [0, 255] 等有限整数范围内，实现从浮点到定点的近似表示。
2. **量化推理阶段**：推理引擎直接以 INT8 整数算术完成矩阵乘法等核心运算（充分利用 CPU/GPU/NPU 的 SIMD 整数指令集），仅在必要节点（如输出层或后续浮点算子）进行反量化恢复为 FP32/FP16，从而保持与原模型接口兼容

量化方案：

推理框架按照量化阶段的不同，其模型量化功能分为以下两种：

- Post-training quantization `PTQ`（训练后量化、离线量化）；
- Quantization-aware training `QAT`（训练时量化，伪量化，在线量化）。

**PTQ方法**

**训练量化（Post-Training Quantization, PTQ）**，又称**离线量化**，是一种在模型训练完成后对权重和激活值进行量化的方法。其目标是在不进行额外微调的前提下，通过降低数值精度来减少模型内存占用并加速推理。PTQ 的核心思想是：利用一个小型校准数据集对浮点模型进行统计分析，从而确定合适的量化参数（如缩放因子和零点）。

根据**量化零点**（zero point）是否为 0，PTQ 可分为两类：

- **对称量化**（Symmetric Quantization）：强制零点为 0（即$$x_{\text{zero}} = $$），适用于张量分布近似对称的情形。该方法实现简单、计算高效。
- **非对称量化**（Asymmetric Quantization）：允许零点非零，能够更灵活地拟合实际数据分布，通常在精度保留方面表现更优。

此外，根据**量化粒度**（granularity）的不同，PTQ 还可进一步划分为：

- **逐层量化**（Layer-wise Quantization）：整层的权重或激活共享同一组量化参数（缩放因子$S$ 和零点$Z$）。例如，NVIDIA/TensorRT 广泛采用此策略，每层使用单一阈值进行量化，在保持高推理效率的同时显著降低开销。
- **逐通道量化**（Channel-wise Quantization）：为每个输出通道独立计算量化参数（即每个通道拥有专属的 $S_c$ 与 $Z_c$），特别适用于卷积层等具有明显通道间差异的结构，可显著提升量化后的模型精度。